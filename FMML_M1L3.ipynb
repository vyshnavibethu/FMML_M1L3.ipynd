{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vyshnavibethu/FMML_M1L3.ipynd/blob/main/FMML_M1L3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3yfry25JgZK"
      },
      "source": [
        "# Data Augmentation\n",
        "\n",
        "FMML Module 1, Lab 3\n",
        "\n",
        "In this lab, we will see how augmentation of data samples help in improving the machine learning performance. Augmentation is the process of creating new data samples by making reasonable modifications to the original data samples. This is particularly useful when the size of the training data is small. We will use the MNISt dataset for this lab. We will also reuse functions from the previous labs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xZU8_elooqP0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from sklearn.utils.extmath import cartesian\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJvmWJ58ovx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be5e12e-3c22-430d-9bb5-20d32c4db2ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "# normalizing the data\n",
        "train_X = train_X / 255\n",
        "test_X = test_X / 255\n",
        "\n",
        "# subsample from images and labels. Otherwise it will take too long!\n",
        "train_X = train_X[::1200, :, :].copy()\n",
        "train_y = train_y[::1200].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XamH6z1Rt7S"
      },
      "source": [
        "Let us borrow a few functions from the previous labs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk2W5_3BRLMS"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    traindata = traindata.reshape(-1, 28*28)\n",
        "    testdata = testdata.reshape(-1, 28*28)\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel\n",
        "\n",
        "\n",
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGiA3LmDSJZo"
      },
      "source": [
        "In this lab, we will use the image pixels themselves as features, instead of extracting features. Each image has 28*28 pixels, so we will flatten them to 784 pixels to use as features. Note that this is very compute intensive and will take a long time. Let us first check the baseline accuracy on the test set without any augmentations. We hope that adding augmentations will help us to get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tQvnoasRNEV"
      },
      "outputs": [],
      "source": [
        "testpred = NN(train_X, train_y, test_X)\n",
        "print(\"Baseline accuracy without augmentation:\",\n",
        "      Accuracy(test_y, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfkcMfhIZQ7U"
      },
      "source": [
        "Let us try to improve this accuracy using augmentations. When we create augmentations, we have to make sure that the changes reflect what will naturally occur in the dataset. For example, we should not add colour to our samples as an augmentation because they do not naturally occur. We should not also flip the images in MNIST, because flipped images have different meanings for digits. So, we will use the following augmentations:\n",
        "\n",
        "### Augmentation 1: Rotation\n",
        "\n",
        "Let us try rotating the image a little. We will use the `rotate` function from the `skimage` module. We will rotate the image by 10 degrees and -10 degrees. Rotation is a reasonable augmentation because the digit will still be recognizable even after rotation and is representative of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5WolJ9fZE7L"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axs[0].imshow(train_X[2], cmap=\"gray\")\n",
        "axs[0].set_title(\"Original Image\")\n",
        "\n",
        "axs[1].imshow(rotate(train_X[2], 10), cmap=\"gray\")\n",
        "axs[1].set_title(\"Rotate +10 degrees\")\n",
        "\n",
        "axs[2].imshow(rotate(train_X[2], -10), cmap=\"gray\")\n",
        "axs[2].set_title(\"Rotate -10 degrees\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE33Yxgggu0c"
      },
      "source": [
        "After rotating, the the class of the image is still the same. Let us make a function to rotate multiple images by random angles. We want a slightly different image every time we run this function. So, we generate a random number between 0 and 1 and change it so that it lies between -constraint/2 and +constraint/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyM7pUV7Reze"
      },
      "outputs": [],
      "source": [
        "def augRotate(sample, angleconstraint):\n",
        "    \"\"\"\n",
        "    This function takes in a sample and an angle constraint and returns the augmented sample\n",
        "    by rotating the sample by a random angle within the angle constraint\n",
        "\n",
        "    sample: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    angleconstraint: the maximum angle by which the sample can be rotated\n",
        "\n",
        "    returns: the augmented sample which is the input sample rotated by a random angle within the angle constraint\n",
        "    \"\"\"\n",
        "    if angleconstraint == 0:\n",
        "        return sample\n",
        "    if len(sample.shape) == 2:\n",
        "        # make sure the sample is 3 dimensional\n",
        "        sample = np.expand_dims(sample, 0)\n",
        "    angle = rng.random(len(sample))  # generate random numbers for angles\n",
        "    # make the random angle constrained\n",
        "    angle = (angle - 0.5) * angleconstraint\n",
        "    nsample = sample.copy()  # preallocate the augmented array to make it faster\n",
        "    for ii in range(len(sample)):\n",
        "        nsample[ii] = rotate(sample[ii], angle[ii])\n",
        "    return np.squeeze(nsample)  # take care if the input had only one sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDk-N5VNjar9"
      },
      "source": [
        "This function returns a slightly different image each time we call it. So we can increase the number of images in the sample by any multiple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw3O9zGFgI8K"
      },
      "outputs": [],
      "source": [
        "sample = train_X[20]\n",
        "angleconstraint = 70\n",
        "\n",
        "fig, axs = plt.subplots(1, 5, figsize=(15, 5))\n",
        "\n",
        "axs[0].imshow(sample, cmap=\"gray\")\n",
        "axs[0].set_title(\"Original Image\")\n",
        "\n",
        "axs[1].imshow(augRotate(sample, angleconstraint), cmap=\"gray\")\n",
        "axs[1].set_title(\"Aug. Sample 1\")\n",
        "\n",
        "axs[2].imshow(augRotate(sample, angleconstraint), cmap=\"gray\")\n",
        "axs[2].set_title(\"Aug. Sample 2\")\n",
        "\n",
        "axs[3].imshow(augRotate(sample, angleconstraint), cmap=\"gray\")\n",
        "axs[3].set_title(\"Aug. Sample 3\")\n",
        "\n",
        "axs[4].imshow(augRotate(sample, angleconstraint), cmap=\"gray\")\n",
        "axs[4].set_title(\"Aug. Sample 4\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytv3NxF-kgxN"
      },
      "source": [
        "Let us augment the whole dataset and see if this improves the test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNzNAoDBkRzj"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "angleconstraint = 60\n",
        "naugmentations = 5\n",
        "\n",
        "# augment\n",
        "augdata = train_X  # we include the original images also in the augmented dataset\n",
        "auglabel = train_y\n",
        "for ii in range(naugmentations):\n",
        "    augdata = np.concatenate(\n",
        "        (augdata, augRotate(train_X, angleconstraint))\n",
        "    )  # concatenate the augmented data to the set\n",
        "    auglabel = np.concatenate(\n",
        "        (auglabel, train_y)\n",
        "    )  # the labels don't change when we augment\n",
        "\n",
        "# check the test accuracy\n",
        "testpred = NN(augdata, auglabel, test_X)\n",
        "print(\"Accuracy after rotation augmentation:\", Accuracy(test_y, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E88Nt9s1p5R6"
      },
      "source": [
        "We can notice a 3-4% improvement compared to non-augmented version of the dataset!\n",
        "\n",
        "The angle constraint is a hyperparameter which we have to tune using a validation set. (Here we are not doing that for time constraints). Let us try a grid search to find the best angle constraint. We will try angles between 0 and 90 degrees. We can also try different multiples of the original dataset. We will use the best hyperparameters to train the model and check the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiaFRLREmGp6"
      },
      "outputs": [],
      "source": [
        "angleconstraints = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]  # the values we want to test\n",
        "accuracies = np.zeros(\n",
        "    len(angleconstraints), dtype=float\n",
        ")  # we will save the values here\n",
        "\n",
        "for ii in range(len(angleconstraints)):\n",
        "    # create the augmented dataset\n",
        "    augdata = train_X  # we include the original images also in the augmented dataset\n",
        "    auglabel = train_y\n",
        "    for jj in range(naugmentations):\n",
        "        augdata = np.concatenate(\n",
        "            (augdata, augRotate(train_X, angleconstraints[ii]))\n",
        "        )  # concatenate the augmented data to the set\n",
        "        auglabel = np.concatenate(\n",
        "            (auglabel, train_y)\n",
        "        )  # the labels don't change when we augment\n",
        "\n",
        "    # check the test accuracy\n",
        "    testpred = NN(augdata, auglabel, test_X)\n",
        "    accuracies[ii] = Accuracy(test_y, testpred)\n",
        "    print(\n",
        "        \"Accuracy after rotation augmentation constrained by\",\n",
        "        angleconstraints[ii],\n",
        "        \"degrees is\",\n",
        "        accuracies[ii]*100,\n",
        "        \"%\",\n",
        "        flush=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oVDRYP2rxob"
      },
      "source": [
        "Let us see the best value for angle constraint: (Ideally this should be done on validation set, not test set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqthJa_pmMHz"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
        "# plot the variation of accuracy\n",
        "ax.plot(angleconstraints, accuracies)\n",
        "ax.set_xlabel(\"angle\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "\n",
        "# plot the maximum accuracy\n",
        "maxind = np.argmax(accuracies)\n",
        "plt.scatter(angleconstraints[maxind], accuracies[maxind], c=\"red\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ8YuVfCuGTj"
      },
      "source": [
        "### Augmentation 2: Shear\n",
        "\n",
        "\n",
        "Let us try one more augmentation: shear. Shear is the transformation of an image in which the x-coordinate of all points is shifted by an amount proportional to the y-coordinate of the point. We will use the `AffineTransform` function from the `skimage` module to shear the image by a small amount between two numbers. We will use the same naive grid search method to find the best hyperparameters for shear. We will use the best hyperparameters to train the model and check the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMiw46NLwssK"
      },
      "outputs": [],
      "source": [
        "def shear(sample, amount):\n",
        "    \"\"\"\n",
        "    This function takes in a sample and an amount and returns the augmented sample\n",
        "    by shearing the sample by the given amount\n",
        "\n",
        "    sample: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    amount: the amount by which the sample should be sheared\n",
        "\n",
        "    returns: the augmented sample which is the input sample sheared by the given amount\n",
        "    \"\"\"\n",
        "    tform = AffineTransform(shear=amount)\n",
        "    img = warp(sample, tform)\n",
        "\n",
        "    # Applying shear makes the digit off-center\n",
        "    # Since all images are centralized, we will do the same here\n",
        "    col = img.sum(0).nonzero()[0]\n",
        "    row = img.sum(1).nonzero()[0]\n",
        "    if len(col) > 0 and len(row) > 0:\n",
        "        xshift = int(sample.shape[0] / 2 - (row[0] + row[-1]) / 2)\n",
        "        yshift = int(sample.shape[1] / 2 - (col[0] + col[-1]) / 2)\n",
        "        img = np.roll(img, (xshift, yshift), (0, 1))\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_u_EYpmnABK"
      },
      "outputs": [],
      "source": [
        "sample = train_X[2]\n",
        "fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
        "\n",
        "axs[0].imshow(sample, cmap=\"gray\")\n",
        "axs[0].set_title(\"Original Image\")\n",
        "\n",
        "axs[1].imshow(shear(sample, 0.2), cmap=\"gray\")\n",
        "axs[1].set_title(\"Amount = 0.2\")\n",
        "\n",
        "axs[2].imshow(shear(sample, 0.4), cmap=\"gray\")\n",
        "axs[2].set_title(\"Amount = 0.4\")\n",
        "\n",
        "axs[3].imshow(shear(sample, 0.6), cmap=\"gray\")\n",
        "axs[3].set_title(\"Amount = 0.6\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGnWMoyM2pK4"
      },
      "source": [
        "Create an augmentation function which applies a random shear according to the constraint we provide:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qLDJyGytwP5"
      },
      "outputs": [],
      "source": [
        "def augShear(sample, shearconstraint):\n",
        "    \"\"\"\n",
        "    This function takes in a sample and a shear constraint and returns the augmented sample\n",
        "    by shearing the sample by a random amount within the shear constraint\n",
        "\n",
        "    sample: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    shearconstraint: the maximum shear by which the sample can be sheared\n",
        "\n",
        "    returns: the augmented sample which is the input sample sheared by a random amount within the shear constraint\n",
        "    \"\"\"\n",
        "    if shearconstraint == 0:\n",
        "        return sample\n",
        "    if len(sample.shape) == 2:\n",
        "        # make sure the sample is 3 dimensional\n",
        "        sample = np.expand_dims(sample, 0)\n",
        "    amt = rng.random(len(sample))  # generate random numbers for shear\n",
        "    amt = (amt - 0.5) * shearconstraint  # make the random shear constrained\n",
        "    nsample = sample.copy()  # preallocate the augmented array to make it faster\n",
        "    for ii in range(len(sample)):\n",
        "        nsample[ii] = shear(sample[ii], amt[ii])\n",
        "    return np.squeeze(nsample)  # take care if the input had only one sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6lQcWW93suJ"
      },
      "source": [
        "Let us do a grid search to find the best shear constraint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_wrqPkrzBb_"
      },
      "outputs": [],
      "source": [
        "shearconstraints = [\n",
        "    0,\n",
        "    0.2,\n",
        "    0.4,\n",
        "    0.6,\n",
        "    0.8,\n",
        "    1.0,\n",
        "    1.2,\n",
        "    1.4,\n",
        "    1.6,\n",
        "    1.8,\n",
        "    2.0,\n",
        "]  # the values we want to test\n",
        "accuracies = np.zeros(\n",
        "    len(shearconstraints), dtype=float\n",
        ")  # we will save the values here\n",
        "\n",
        "for ii in range(len(shearconstraints)):\n",
        "    # create the augmented dataset\n",
        "    augdata = train_X  # we include the original images also in the augmented dataset\n",
        "    auglabel = train_y\n",
        "    for jj in range(naugmentations):\n",
        "        augdata = np.concatenate(\n",
        "            (augdata, augShear(train_X, shearconstraints[ii]))\n",
        "        )  # concatenate the augmented data to the set\n",
        "        auglabel = np.concatenate(\n",
        "            (auglabel, train_y)\n",
        "        )  # the labels don't change when we augment\n",
        "\n",
        "    # check the test accuracy\n",
        "    testpred = NN(augdata, auglabel, test_X)\n",
        "    accuracies[ii] = Accuracy(test_y, testpred)\n",
        "    print(\n",
        "        \"Accuracy after shear augmentation constrained by\",\n",
        "        shearconstraints[ii],\n",
        "        \"is\",\n",
        "        accuracies[ii]*100,\n",
        "        \"%\",\n",
        "        flush=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKaH-YR-zVnA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
        "# plot the variation of accuracy\n",
        "ax.plot(shearconstraints, accuracies)\n",
        "ax.set_xlabel(\"angle\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "\n",
        "# plot the maximum accuracy\n",
        "maxind = np.argmax(accuracies)\n",
        "plt.scatter(shearconstraints[maxind], accuracies[maxind], c=\"red\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccfdbRcQ7Zgg"
      },
      "source": [
        "### Augmentation 3: Rotation + Shear\n",
        "\n",
        "\n",
        "\n",
        "We can do multiple augmentations at the same time. Here is a function to do both shear and rotation to the sample. In this case, we will have two hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh8S_Pxa0XCv"
      },
      "outputs": [],
      "source": [
        "def augRotateShear(sample, angleconstraint, shearconstraint):\n",
        "    \"\"\"\n",
        "    This function takes in a sample, an angle constraint and a shear constraint and returns the augmented sample\n",
        "    by rotating the sample by a random angle within the angle constraint and shearing the sample by a random amount within the shear constraint\n",
        "\n",
        "    sample: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    angleconstraint: the maximum angle by which the sample can be rotated\n",
        "    shearconstraint: the maximum shear by which the sample can be sheared\n",
        "\n",
        "    returns: the augmented sample which is the input sample rotated by a random angle within the angle constraint and sheared by a random amount within the shear constraint\n",
        "    \"\"\"\n",
        "    if len(sample.shape) == 2:\n",
        "        # make sure the sample is 3 dimensional\n",
        "        sample = np.expand_dims(sample, 0)\n",
        "    amt = rng.random(len(sample))  # generate random numbers for shear\n",
        "    amt = (amt - 0.5) * shearconstraint  # make the random shear constrained\n",
        "    angle = rng.random(len(sample))  # generate random numbers for angles\n",
        "    # make the random angle constrained\n",
        "    angle = (angle - 0.5) * angleconstraint\n",
        "    nsample = sample.copy()  # preallocate the augmented array to make it faster\n",
        "    for ii in range(len(sample)):\n",
        "        nsample[ii] = rotate(\n",
        "            shear(sample[ii], amt[ii]), angle[ii]\n",
        "        )  # first apply shear, then rotate\n",
        "    return np.squeeze(nsample)  # take care if the input had only one sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGKyjjNx-NQ4"
      },
      "source": [
        "Since we have two hyperparameters, we have to do the grid search on a 2 dimensional matrix. We can use our previous experience to inform where to search for the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJC45WRg0pOP"
      },
      "outputs": [],
      "source": [
        "shearconstraints = [\n",
        "    0,\n",
        "    0.2,\n",
        "    0.4,\n",
        "    0.6,\n",
        "    0.8,\n",
        "    1.0,\n",
        "    1.2,\n",
        "    1.4,\n",
        "    1.6,\n",
        "]  # the values we want to test\n",
        "angleconstraints = [0, 10, 20, 30, 40, 50, 60]  # the values we want to test\n",
        "# cartesian product of both\n",
        "hyp = cartesian((shearconstraints, angleconstraints))\n",
        "\n",
        "accuracies = np.zeros(len(hyp), dtype=float)  # we will save the values here\n",
        "\n",
        "for ii in range(len(hyp)):\n",
        "    # create the augmented dataset\n",
        "    augdata = train_X  # we include the original images also in the augmented dataset\n",
        "    auglabel = train_y\n",
        "    for jj in range(naugmentations):\n",
        "        augdata = np.concatenate(\n",
        "            (augdata, augRotateShear(train_X, hyp[ii][0], hyp[ii][1]))\n",
        "        )  # concatenate the augmented data to the set\n",
        "        auglabel = np.concatenate(\n",
        "            (auglabel, train_y)\n",
        "        )  # the labels don't change when we augment\n",
        "\n",
        "    # check the test accuracy\n",
        "    testpred = NN(augdata, auglabel, test_X)\n",
        "    accuracies[ii] = Accuracy(test_y, testpred)\n",
        "    print(\n",
        "        \"Accuracy after augmentation shear:\",\n",
        "        hyp[ii][0],\n",
        "        \"angle:\",\n",
        "        hyp[ii][1],\n",
        "        \"is\",\n",
        "        accuracies[ii]*100,\n",
        "        \"%\",\n",
        "        flush=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT6CnvSDEX7a"
      },
      "source": [
        "Let us plot it two dimensionally to see which is the best value for the hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD2i7msI_cLd"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
        "im = ax.imshow(\n",
        "    accuracies.reshape((len(shearconstraints), len(angleconstraints))), cmap=\"hot\"\n",
        ")\n",
        "ax.set_xlabel(\"Angle\")\n",
        "ax.set_ylabel(\"Shear\")\n",
        "ax.set_xticks(np.arange(len(angleconstraints)))\n",
        "ax.set_xticklabels(angleconstraints)\n",
        "ax.set_yticks(np.arange(len(shearconstraints)))\n",
        "ax.set_yticklabels(shearconstraints)\n",
        "plt.colorbar(im)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHcZWJiFJDMh"
      },
      "source": [
        "It seems that rotation and shear don't mix! The best accuracy is when rotation is zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAasQo1C3x4A"
      },
      "source": [
        "## Questions\n",
        "Try these questions for better understanding. You may not be able to solve all of them.\n",
        "1. What is the best value for angle constraint and shear constraint you got? How much did the accuracy improve as compared to not using augmentations?\n",
        "The optimal values for angle and shear constraints depend on the specific dataset and the nature of the task. Typical ranges are:\n",
        "\n",
        "Angle Constraint:\n",
        "1\n",
        "0\n",
        "∘\n",
        "10\n",
        "∘\n",
        "  to\n",
        "3\n",
        "0\n",
        "∘\n",
        "30\n",
        "∘\n",
        "  for rotation. Beyond\n",
        "3\n",
        "0\n",
        "∘\n",
        "30\n",
        "∘\n",
        " , the augmented images might become unrealistic, especially for tasks sensitive to spatial orientation.\n",
        "Shear Constraint: Values in the range of 0.1 to 0.3 (for small shear distortions) work well. Larger values may distort the images excessively.\n",
        "On average, using these augmentations can improve accuracy by 5% to 15% compared to not using augmentations. The exact improvement depends on the dataset size and complexity.\n",
        "2. Can you increase the accuracy by increasing the number of augmentations from each sample?\n",
        "The optimal values for angle and shear constraints depend on the specific dataset and the nature of the task. Typical ranges are:\n",
        "\n",
        "Angle Constraint:\n",
        "1\n",
        "0\n",
        "∘\n",
        "10\n",
        "∘\n",
        "  to\n",
        "3\n",
        "0\n",
        "∘\n",
        "30\n",
        "∘\n",
        "  for rotation. Beyond\n",
        "3\n",
        "0\n",
        "∘\n",
        "30\n",
        "∘\n",
        " , the augmented images might become unrealistic, especially for tasks sensitive to spatial orientation.\n",
        "Shear Constraint: Values in the range of 0.1 to 0.3 (for small shear distortions) work well. Larger values may distort the images excessively.\n",
        "On average, using these augmentations can improve accuracy by 5% to 15% compared to not using augmentations. The exact improvement depends on the dataset size and complexity.\n",
        "3. Try implementing a few augmentations of your own and experimenting with them. A good reference is <a href=https://www.analyticsvidhya.com/blog/2019/12/image-augmentation-deep-learning-pytorch/>here. </a>\n",
        "Here are a few custom augmentations that can be implemented and experimented with:\n",
        "\n",
        "CutMix: Overlay patches of one image onto another and adjust labels proportionally.\n",
        "GridMask: Overlay grid-like masks on images to occlude random sections.\n",
        "Channel Shuffle: Shuffle the RGB channels of the image.\n",
        "Random Erasing: Randomly occlude parts of the image with a blank patch.\n",
        "Color Jitter Variants: Alter brightness, contrast, saturation, and hue with custom parameter ranges.\n",
        "Each of these augmentations introduces unique variations that can improve generalization. Combining them with existing techniques can yield better results.\n",
        "\n",
        "\n",
        "4. Try combining various augmentations. What is the highest accuracy you can get? What is the smallest training dataset you can take and still get accuracy above 50%?\n",
        "Combining Augmentations:\n",
        "Using combinations like random rotation, shear, horizontal/vertical flips, brightness adjustments, and CutMix often yields the best results. For instance:\n",
        "\n",
        "Combine moderate angle and shear constraints with flips and brightness jitter.\n",
        "Add noise or occlusion techniques like Random Erasing to simulate real-world variations.\n",
        "Smallest Dataset for 50% Accuracy:\n",
        "Using aggressive augmentation, you can significantly reduce the training dataset size while maintaining acceptable performance. For many tasks:\n",
        "\n",
        "A training set size of 10-20% of the original data combined with strong augmentations can yield over 50% accuracy.\n",
        "Leave-one-out cross-validation or stratified sampling ensures efficient data usage.\n",
        "However, achieving the exact result depends on the specific dataset, model, and augmentation setup.\n",
        "\n",
        "Whenever you do any experiment, a good practice is to vary the hyperparameters gradually and create a graph of your results, like we did for gridsearch."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Exercise: Try to take 50 images of each digit and calculate the performance on test set."
      ],
      "metadata": {
        "id": "zu590B33-Xp_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}